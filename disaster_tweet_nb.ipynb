{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Importing Dataset\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "# import string\n",
    "# from collections import deque\n",
    "# \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# \n",
    "# # data\n",
    "# from sklearn import datasets\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# \n",
    "# # Feature selection\n",
    "# from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.impute import SimpleImputer\n",
    "# \n",
    "# # classifiers / models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# \n",
    "# # other\n",
    "# from sklearn.metrics import (\n",
    "#     accuracy_score,\n",
    "#     log_loss,\n",
    "#     make_scorer,\n",
    "#     mean_squared_error,\n",
    "#     precision_score,\n",
    "#     recall_score,\n",
    "#     f1_score\n",
    "# )\n",
    "from sklearn.model_selection import (\n",
    "#     GridSearchCV,\n",
    "#     RandomizedSearchCV,\n",
    "#     ShuffleSplit,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    OneHotEncoder,\n",
    "#     OrdinalEncoder,\n",
    "#     PolynomialFeatures,\n",
    "#     StandardScaler\n",
    ")\n",
    "# from sklearn.svm import SVC, SVR\n",
    "chosen_seed = 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Data\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1609</th>\n",
       "      <td>collapse</td>\n",
       "      <td>Chicago, Illinois</td>\n",
       "      <td>Only one commodity has escaped the total colla...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1740</th>\n",
       "      <td>collision</td>\n",
       "      <td>'soooota</td>\n",
       "      <td>@Zojadelin you literally almost had a head on ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3223</th>\n",
       "      <td>emergency%20services</td>\n",
       "      <td>Nevada, USA</td>\n",
       "      <td>Can you recommend anyone for this #job? RN Eme...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>apocalypse</td>\n",
       "      <td>Oakland</td>\n",
       "      <td>Julie + R is the apocalypse version of Romeo +...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2515</th>\n",
       "      <td>desolation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @FreeDiscountBks: **Desolation Run** #FREE ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   keyword           location  \\\n",
       "1609              collapse  Chicago, Illinois   \n",
       "1740             collision           'soooota   \n",
       "3223  emergency%20services        Nevada, USA   \n",
       "282             apocalypse            Oakland   \n",
       "2515            desolation                NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "1609  Only one commodity has escaped the total colla...       0  \n",
       "1740  @Zojadelin you literally almost had a head on ...       1  \n",
       "3223  Can you recommend anyone for this #job? RN Eme...       0  \n",
       "282   Julie + R is the apocalypse version of Romeo +...       0  \n",
       "2515  RT @FreeDiscountBks: **Desolation Run** #FREE ...       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### BEGIN STARTER CODE\n",
    "\n",
    "dataset_df = pd.read_csv(\"data/train.csv\", usecols=[\"keyword\", \"location\", \"text\", \"target\"])\n",
    "train_df, test_df = train_test_split(dataset_df, test_size=0.2, random_state=chosen_seed)\n",
    "train_df.head()\n",
    "\n",
    "### BEGIN STARTER CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_feature = \"text\"\n",
    "target = \"target\"\n",
    "\n",
    "X_train, y_train = train_df.drop(columns=[\"target\", \"location\"]), train_df[target]\n",
    "X_test, y_test = test_df.drop(columns=[\"target\", \"location\"]), test_df[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. EDA\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have a class imbalance scenario.\n",
    "- We want to predict as much positive cases as possible as this will help us to act preemptively to take measures in case if a disaster is eminent. However, too much of false positive can create unnecessary panic and people might not take the threat predictions seriously in future. Hence, we will be using **f1-score** as our evaluation metrics, so that we maximize precision and recall together, assuming that this threat prediction model will be used for both general and security purposes\n",
    "    - However, if we are creating this model for security agencies, recall might be a better option as false positive won't hurt other than taking some extra precaution.\n",
    "- We will be using `keyword` feature as categorical feature and we can apply one-hot encoding\n",
    "- We can apply CountVectorizer on `text` feature which is already used in the starter code\n",
    "- We will ignore `location` feature for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>count</th>\n",
       "      <th>perc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3500</td>\n",
       "      <td>0.574713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2590</td>\n",
       "      <td>0.425287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target  count      perc\n",
       "0       0   3500  0.574713\n",
       "1       1   2590  0.425287"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### YOUR ANSWER HERE\n",
    "class_dist = train_df.value_counts(\"target\").reset_index(name=\"count\")\n",
    "class_dist[\"perc\"] = class_dist[\"count\"]/train_df.shape[0]\n",
    "class_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique keywords: 222\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of unique keywords: {len(train_df.keyword.unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Housekeeping\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "def summarize_cv_scores(X, classifier_name):\n",
    "    \"\"\"\n",
    "    This function summarizes the output of cross_validate function \n",
    "    from sklearn.model_selection and provides the mean and \n",
    "    standard deviation of all columns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : dict\n",
    "        The output of cross_validate function from sklearn.model_selection.\n",
    "\n",
    "    classifier_name : string\n",
    "        Name of the classifier\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    Constructing toy example for X dictionary.\n",
    "\n",
    "    >>> toy_score = {\n",
    "        \"fit_time\": np.array([0.1, 0.2, 0.3, 0.4, 0.5]),\n",
    "        \"score_time\": np.array([1, 2, 3, 4, 5]),\n",
    "        \"test_accuracy\": np.array([0.5, 0.5, 0.5, 0.5, 0.5]),\n",
    "        \"train_accuracy\": np.array([0.5, 0.5, 0.5, 0.5, 0.5]),\n",
    "        \"test_f1\": np.array([0.1, 0.1, 0.2, 0.1, 0.1]),\n",
    "        \"train_f1\": np.array([0.1, 0.3, 0.1, 0.1, 0.1]),\n",
    "    }\n",
    "    \n",
    "    Using the function\n",
    "    \n",
    "    >>> summarize_cv_scores(toy_score, \"toy_test\")\n",
    "    \"\"\"\n",
    "    X_df = pd.DataFrame(X)\n",
    "    col_names = (\n",
    "        pd.Series(X_df.columns.tolist()).str.replace(\"test_\", \"validation_\").tolist()\n",
    "    )\n",
    "    col_names = [f\"{t}_{i}\" for t in [\"mean\", \"std\"] for i in col_names]\n",
    "    X_df = pd.DataFrame(pd.concat([X_df.mean(), X_df.std()])).T\n",
    "    X_df.columns = col_names\n",
    "    X_df[\"classifier_name\"] = classifier_name\n",
    "    col_names = [\"classifier_name\"] + col_names\n",
    "    return X_df[col_names]\n",
    "\n",
    "\n",
    "toy_score = {\n",
    "    \"fit_time\": np.array([0.1, 0.2, 0.3, 0.4, 0.5]),\n",
    "    \"score_time\": np.array([1, 2, 3, 4, 5]),\n",
    "    \"test_accuracy\": np.array([0.5, 0.5, 0.5, 0.5, 0.5]),\n",
    "    \"train_accuracy\": np.array([0.5, 0.5, 0.5, 0.5, 0.5]),\n",
    "    \"test_f1\": np.array([0.1, 0.1, 0.2, 0.1, 0.1]),\n",
    "    \"train_f1\": np.array([0.1, 0.3, 0.1, 0.1, 0.1]),\n",
    "}\n",
    "\n",
    "expected = {\n",
    "    \"classifier_name\": [\"toy_test\"],\n",
    "    \"mean_fit_time\": [0.3],\n",
    "    \"mean_score_time\": [3],\n",
    "    \"mean_validation_accuracy\": [0.5],\n",
    "    \"mean_train_accuracy\": [0.5],\n",
    "    \"mean_validation_f1\": [0.12],\n",
    "    \"mean_train_f1\": [0.14],\n",
    "    \"std_fit_time\": [0.158114],\n",
    "    \"std_score_time\": [1.581139],\n",
    "    \"std_validation_accuracy\": [0.0],\n",
    "    \"std_train_accuracy\": [0.0],\n",
    "    \"std_validation_f1\": [0.044721],\n",
    "    \"std_train_f1\": [0.089443],\n",
    "}\n",
    "\n",
    "\n",
    "assert isinstance(\n",
    "    summarize_cv_scores(toy_score, \"toy_test\"), pd.DataFrame\n",
    "), \"Check data structure\"\n",
    "assert (\n",
    "    int(\n",
    "        (\n",
    "            np.round(summarize_cv_scores(toy_score, \"toy_test\"), 4)\n",
    "            == np.round(pd.DataFrame(data=expected), 4)\n",
    "        ).T.sum()\n",
    "    )\n",
    "    == 13\n",
    "), \"Check function logic\"\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Baseline model\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier_name</th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_validation_f1</th>\n",
       "      <th>mean_train_f1</th>\n",
       "      <th>mean_validation_accuracy</th>\n",
       "      <th>mean_train_accuracy</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_validation_f1</th>\n",
       "      <th>std_train_f1</th>\n",
       "      <th>std_validation_accuracy</th>\n",
       "      <th>std_train_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dummy Classifier (stratified)</td>\n",
       "      <td>0.00112</td>\n",
       "      <td>0.002309</td>\n",
       "      <td>0.440613</td>\n",
       "      <td>0.437563</td>\n",
       "      <td>0.520525</td>\n",
       "      <td>0.516872</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.013341</td>\n",
       "      <td>0.00522</td>\n",
       "      <td>0.011435</td>\n",
       "      <td>0.004484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 classifier_name  mean_fit_time  mean_score_time  \\\n",
       "0  Dummy Classifier (stratified)        0.00112         0.002309   \n",
       "\n",
       "   mean_validation_f1  mean_train_f1  mean_validation_accuracy  \\\n",
       "0            0.440613       0.437563                  0.520525   \n",
       "\n",
       "   mean_train_accuracy  std_fit_time  std_score_time  std_validation_f1  \\\n",
       "0             0.516872      0.000139        0.000131           0.013341   \n",
       "\n",
       "   std_train_f1  std_validation_accuracy  std_train_accuracy  \n",
       "0       0.00522                 0.011435            0.004484  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dummy = DummyClassifier(strategy=\"stratified\", random_state=chosen_seed)\n",
    "scores = cross_validate(\n",
    "    model_dummy,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    scoring=[\"f1\", \"accuracy\"],\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True,\n",
    ")\n",
    "\n",
    "mean_scores_df = summarize_cv_scores(scores, \"Dummy Classifier (stratified)\")\n",
    "mean_scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Preprocessing\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = make_column_transformer((CountVectorizer(max_features=20_000, stop_words=\"english\"), \"text\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Logistic Regression\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier_name</th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_validation_f1</th>\n",
       "      <th>mean_train_f1</th>\n",
       "      <th>mean_validation_accuracy</th>\n",
       "      <th>mean_train_accuracy</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_validation_f1</th>\n",
       "      <th>std_train_f1</th>\n",
       "      <th>std_validation_accuracy</th>\n",
       "      <th>std_train_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dummy Classifier (stratified)</td>\n",
       "      <td>0.001120</td>\n",
       "      <td>0.002309</td>\n",
       "      <td>0.440613</td>\n",
       "      <td>0.437563</td>\n",
       "      <td>0.520525</td>\n",
       "      <td>0.516872</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.013341</td>\n",
       "      <td>0.005220</td>\n",
       "      <td>0.011435</td>\n",
       "      <td>0.004484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression (without keyword)</td>\n",
       "      <td>0.274897</td>\n",
       "      <td>0.018164</td>\n",
       "      <td>0.739680</td>\n",
       "      <td>0.972549</td>\n",
       "      <td>0.787028</td>\n",
       "      <td>0.976888</td>\n",
       "      <td>0.023235</td>\n",
       "      <td>0.001234</td>\n",
       "      <td>0.018167</td>\n",
       "      <td>0.001804</td>\n",
       "      <td>0.014694</td>\n",
       "      <td>0.001526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         classifier_name  mean_fit_time  mean_score_time  \\\n",
       "0          Dummy Classifier (stratified)       0.001120         0.002309   \n",
       "0  Logistic Regression (without keyword)       0.274897         0.018164   \n",
       "\n",
       "   mean_validation_f1  mean_train_f1  mean_validation_accuracy  \\\n",
       "0            0.440613       0.437563                  0.520525   \n",
       "0            0.739680       0.972549                  0.787028   \n",
       "\n",
       "   mean_train_accuracy  std_fit_time  std_score_time  std_validation_f1  \\\n",
       "0             0.516872      0.000139        0.000131           0.013341   \n",
       "0             0.976888      0.023235        0.001234           0.018167   \n",
       "\n",
       "   std_train_f1  std_validation_accuracy  std_train_accuracy  \n",
       "0      0.005220                 0.011435            0.004484  \n",
       "0      0.001804                 0.014694            0.001526  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pipe = make_pipeline(\n",
    "    preprocessor,\n",
    "    LogisticRegression(random_state=chosen_seed, class_weight=\"balanced\"),\n",
    ")\n",
    "\n",
    "scores = cross_validate(\n",
    "    model_pipe,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    scoring=[\"f1\", \"accuracy\"],\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True,\n",
    ")\n",
    "\n",
    "mean_scores_df = pd.concat(\n",
    "    [mean_scores_df, summarize_cv_scores(scores, \"Logistic Regression (without keyword)\")]\n",
    ")\n",
    "mean_scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Include _keyword_ feature\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "    OneHotEncoder(handle_unknown=\"ignore\")\n",
    ")\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (CountVectorizer(max_features=20_000, stop_words=\"english\"), \"text\"), (categorical_pipeline, [\"keyword\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipe = make_pipeline(\n",
    "    preprocessor,\n",
    "    LogisticRegression(random_state=chosen_seed, class_weight=\"balanced\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier_name</th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_validation_f1</th>\n",
       "      <th>mean_train_f1</th>\n",
       "      <th>mean_validation_accuracy</th>\n",
       "      <th>mean_train_accuracy</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_validation_f1</th>\n",
       "      <th>std_train_f1</th>\n",
       "      <th>std_validation_accuracy</th>\n",
       "      <th>std_train_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dummy Classifier (stratified)</td>\n",
       "      <td>0.001120</td>\n",
       "      <td>0.002309</td>\n",
       "      <td>0.440613</td>\n",
       "      <td>0.437563</td>\n",
       "      <td>0.520525</td>\n",
       "      <td>0.516872</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.013341</td>\n",
       "      <td>0.005220</td>\n",
       "      <td>0.011435</td>\n",
       "      <td>0.004484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression (without keyword)</td>\n",
       "      <td>0.274897</td>\n",
       "      <td>0.018164</td>\n",
       "      <td>0.739680</td>\n",
       "      <td>0.972549</td>\n",
       "      <td>0.787028</td>\n",
       "      <td>0.976888</td>\n",
       "      <td>0.023235</td>\n",
       "      <td>0.001234</td>\n",
       "      <td>0.018167</td>\n",
       "      <td>0.001804</td>\n",
       "      <td>0.014694</td>\n",
       "      <td>0.001526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression (with keyword)</td>\n",
       "      <td>0.000967</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression (with keyword)</td>\n",
       "      <td>0.380465</td>\n",
       "      <td>0.027819</td>\n",
       "      <td>0.743785</td>\n",
       "      <td>0.972957</td>\n",
       "      <td>0.789163</td>\n",
       "      <td>0.977217</td>\n",
       "      <td>0.059621</td>\n",
       "      <td>0.005877</td>\n",
       "      <td>0.016025</td>\n",
       "      <td>0.001903</td>\n",
       "      <td>0.013231</td>\n",
       "      <td>0.001603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         classifier_name  mean_fit_time  mean_score_time  \\\n",
       "0          Dummy Classifier (stratified)       0.001120         0.002309   \n",
       "0  Logistic Regression (without keyword)       0.274897         0.018164   \n",
       "0     Logistic Regression (with keyword)       0.000967         0.000000   \n",
       "0     Logistic Regression (with keyword)       0.380465         0.027819   \n",
       "\n",
       "   mean_validation_f1  mean_train_f1  mean_validation_accuracy  \\\n",
       "0            0.440613       0.437563                  0.520525   \n",
       "0            0.739680       0.972549                  0.787028   \n",
       "0                 NaN            NaN                       NaN   \n",
       "0            0.743785       0.972957                  0.789163   \n",
       "\n",
       "   mean_train_accuracy  std_fit_time  std_score_time  std_validation_f1  \\\n",
       "0             0.516872      0.000139        0.000131           0.013341   \n",
       "0             0.976888      0.023235        0.001234           0.018167   \n",
       "0                  NaN      0.000125        0.000000                NaN   \n",
       "0             0.977217      0.059621        0.005877           0.016025   \n",
       "\n",
       "   std_train_f1  std_validation_accuracy  std_train_accuracy  \n",
       "0      0.005220                 0.011435            0.004484  \n",
       "0      0.001804                 0.014694            0.001526  \n",
       "0           NaN                      NaN                 NaN  \n",
       "0      0.001903                 0.013231            0.001603  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_validate(\n",
    "    model_pipe,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    scoring=[\"f1\", \"accuracy\"],\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True,\n",
    ")\n",
    "\n",
    "mean_scores_df = pd.concat(\n",
    "    [mean_scores_df, summarize_cv_scores(scores, \"Logistic Regression (with keyword)\")]\n",
    ")\n",
    "mean_scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Our scores (accuracy and f1) using Logistic Regression with keyword feature is better\n",
    "- But, the scores (accuracy and f1) using Logistic Regression with keyword feature is almost similar to the results obtained using Logistic Regression without keyword feature \n",
    "    - This is because keywords are subset of the text and when we use count vectorizer on text, there are chances that we well get a feature with the keyword again. So we are not adding anything significant to our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.6: Adding new features\n",
    "rubric={reasoning:5}\n",
    "\n",
    "Is it possible to further improve the scores? How about adding new features based on our intuitions? \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Name 3 to 4 additional features you think would be helpful in predicting the target. An example would be a binary feature \"has_emoticons\" indicating whether the tweet has emoticons or not. Explain your intuition behind the features and discuss how hard in would be to engineer these features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**solution_1_6_1**\n",
    "\n",
    "### YOUR ANSWER HERE\n",
    "\n",
    "- A binary feature `has_hyperlinks` indicating whether the tweet has hyperlink or not.\n",
    "    - A tweet of a disaster event is likely to have a hyperlink of the news or other related article.\n",
    "- A binary feature `is_valid_location`\n",
    "    - A disaster event is likely to affect a physical location and identifying the location can be of added value\n",
    "- A binary feature `has_numbers`\n",
    "    - A disaster event is likely to have number of people affected in a related tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.7: Extracting your own features \n",
    "rubric={accuracy:4,reasoning:4}\n",
    "\n",
    "In this exercise, we will be adding some very basic length-related and sentiment features.  \n",
    "\n",
    "You will need to install a popular library called `nltk` for this exercise. For that, run the following commands in your `conda` environment. \n",
    "\n",
    "```\n",
    "conda install -c anaconda nltk \n",
    "nltk.download(\"vader_lexicon\")\n",
    "nltk.download(\"punkt\")\n",
    "```        \n",
    "\n",
    "Run the starter code below creates three new features: \n",
    "- Relative character length in the tweet. \n",
    "- Number of words in the tweet.\n",
    "- Sentiment of the tweet (positive (pos), negative (neg), neutral (neu), compound (mixture of different sentiments)). In 571, you carried out sentiment analysis on the IMDB data set. Here we are using some pre-trained machine learning model to extract sentiment expressed in the tweets. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Extract at least two more features that you think might be relevant for prediction and store them as new columns in the train and test sets. Briefly explain your intuition on why these features might help the prediction task. \n",
    "2. Would it have been OK to create new columns directly in the original `df` instead of creating them separately for train and test splits? Would that be violation of the golden rule? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/debananda/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/debananda/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "### BEGIN STARTER CODE\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"vader_lexicon\")\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "### END STARTER CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN STARTER CODE\n",
    "\n",
    "\n",
    "def get_relative_length(text, TWITTER_ALLOWED_CHARS=280.0):\n",
    "    \"\"\"\n",
    "    Returns the relative length of text.\n",
    "\n",
    "    Parameters:\n",
    "    ------\n",
    "    text: (str)\n",
    "    the input text\n",
    "\n",
    "    Keyword arguments:\n",
    "    ------\n",
    "    TWITTER_ALLOWED_CHARS: (float)\n",
    "    the denominator for finding relative length\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    relative length of text: (float)\n",
    "\n",
    "    \"\"\"\n",
    "    return len(text) / TWITTER_ALLOWED_CHARS\n",
    "\n",
    "\n",
    "def get_length_in_words(text):\n",
    "    \"\"\"\n",
    "    Returns the length of the text in words.\n",
    "\n",
    "    Parameters:\n",
    "    ------\n",
    "    text: (str)\n",
    "    the input text\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    length of tokenized text: (int)\n",
    "\n",
    "    \"\"\"\n",
    "    return len(nltk.word_tokenize(text))\n",
    "\n",
    "\n",
    "def get_sentiment(text):\n",
    "    \"\"\"\n",
    "    Returns the maximum scoring sentiment of the text\n",
    "\n",
    "    Parameters:\n",
    "    ------\n",
    "    text: (str)\n",
    "    the input text\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    sentiment of the text: (str)\n",
    "    \"\"\"\n",
    "    scores = sid.polarity_scores(text)\n",
    "    return max(scores, key=lambda x: scores[x])\n",
    "\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN STARTER CODE\n",
    "\n",
    "train_df = train_df.assign(n_words=train_df[\"text\"].apply(get_length_in_words))\n",
    "train_df = train_df.assign(sentiment=train_df[\"text\"].apply(get_sentiment))\n",
    "train_df = train_df.assign(rel_char_len=train_df[\"text\"].apply(get_relative_length))\n",
    "\n",
    "test_df = test_df.assign(n_words=test_df[\"text\"].apply(get_length_in_words))\n",
    "test_df = test_df.assign(sentiment=test_df[\"text\"].apply(get_sentiment))\n",
    "test_df = test_df.assign(rel_char_len=test_df[\"text\"].apply(get_relative_length))\n",
    "\n",
    "### END STARTER CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution_1_7_1\n",
    "\n",
    "### YOUR ANSWER HERE\n",
    "train_df = train_df.assign(\n",
    "    has_hyperlinks=train_df[\"text\"].str.contains(\"http://|https://\").astype(int)\n",
    ")\n",
    "train_df = train_df.assign(\n",
    "    has_numbers=train_df[\"text\"].str.contains(\"[0-9]\").astype(int)\n",
    ")\n",
    "\n",
    "test_df = test_df.assign(\n",
    "    has_hyperlinks=test_df[\"text\"].str.contains(\"http://|https://\").astype(int)\n",
    ")\n",
    "test_df = test_df.assign(\n",
    "    has_numbers=test_df[\"text\"].str.contains(\"[0-9]\").astype(int)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**solution_1_7_1 (reasoning)**\n",
    "\n",
    "### YOUR ANSWER HERE\n",
    "\n",
    "- A binary feature `has_hyperlinks` indicating whether the tweet has hyperlink or not.\n",
    "    - A tweet of a disaster event is likely to have a hyperlink of the news or other related article.\n",
    "- A binary feature `has_numbers`\n",
    "    - A disaster event is likely to have number of people affected in a related tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**solution_1_7_2**\n",
    "\n",
    "### YOUR ANSWER HERE\n",
    "\n",
    "In this case we will not violate the golden rule if we do the transformation in original `df` as all our transformations are row level transformations. However, it is better to avoid doing the transformation in the original df as we will not be losing our original untouched dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Pipeline with all features\n",
    "rubric={accuracy:4,reasoning:2}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Identify different feature types in your new data set with the features you created above, and separate features and targets from your new dataset. \n",
    "2. Define a column transformer for your mixed feature types. Again, set `max_features` of `CountVectorizer` to 20_000.  \n",
    "3. Define a pipeline with the column transformer and `LogisticRegression` with `class_weight` of `LogisticRegression` set to \"balanced\" and report mean cross-validation f1 scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution_1_8_1\n",
    "\n",
    "### YOUR ANSWER HERE\n",
    "X_train, y_train = train_df.drop(columns=[\"target\"]), train_df[target]\n",
    "X_test, y_test = test_df.drop(columns=[\"target\"]), test_df[target]\n",
    "\n",
    "numeric_features = [\"n_words\", \"rel_char_len\"]\n",
    "categorical_features = [\"keyword\", \"sentiment\"]\n",
    "text_features = \"text\"\n",
    "binary_features = [\"has_hyperlinks\", \"has_numbers\"]\n",
    "drop_features = [\"location\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution_1_8_2\n",
    "\n",
    "### YOUR ANSWER HERE\n",
    "numeric_pipeline = make_pipeline(SimpleImputer(), StandardScaler())\n",
    "categorical_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"most_frequent\"), OneHotEncoder(handle_unknown=\"ignore\")\n",
    ")\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (numeric_pipeline, numeric_features),\n",
    "    (SimpleImputer(strategy=\"most_frequent\"), binary_features),\n",
    "    (categorical_pipeline, categorical_features),\n",
    "    (CountVectorizer(max_features=20_000), \"text\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier_name</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>validation_f1</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>validation_accuracy</th>\n",
       "      <th>train_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dummy Classifier (stratified)</td>\n",
       "      <td>0.001346</td>\n",
       "      <td>0.002688</td>\n",
       "      <td>0.178843</td>\n",
       "      <td>0.183351</td>\n",
       "      <td>0.692064</td>\n",
       "      <td>0.695911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression (without keyword)</td>\n",
       "      <td>0.722058</td>\n",
       "      <td>0.043308</td>\n",
       "      <td>0.689627</td>\n",
       "      <td>0.973009</td>\n",
       "      <td>0.883686</td>\n",
       "      <td>0.989666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression (with keyword)</td>\n",
       "      <td>0.982626</td>\n",
       "      <td>0.056809</td>\n",
       "      <td>0.690259</td>\n",
       "      <td>0.973728</td>\n",
       "      <td>0.883686</td>\n",
       "      <td>0.989941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression (feat eng.)</td>\n",
       "      <td>1.439420</td>\n",
       "      <td>0.059701</td>\n",
       "      <td>0.690929</td>\n",
       "      <td>0.972813</td>\n",
       "      <td>0.883576</td>\n",
       "      <td>0.989583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         classifier_name  fit_time  score_time  validation_f1  \\\n",
       "0          Dummy Classifier (stratified)  0.001346    0.002688       0.178843   \n",
       "0  Logistic Regression (without keyword)  0.722058    0.043308       0.689627   \n",
       "0     Logistic Regression (with keyword)  0.982626    0.056809       0.690259   \n",
       "0        Logistic Regression (feat eng.)  1.439420    0.059701       0.690929   \n",
       "\n",
       "   train_f1  validation_accuracy  train_accuracy  \n",
       "0  0.183351             0.692064        0.695911  \n",
       "0  0.973009             0.883686        0.989666  \n",
       "0  0.973728             0.883686        0.989941  \n",
       "0  0.972813             0.883576        0.989583  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution_1_8_3\n",
    "\n",
    "### YOUR ANSWER HERE\n",
    "\n",
    "model_pipe = make_pipeline(\n",
    "    preprocessor,\n",
    "    LogisticRegression(random_state=2, class_weight=\"balanced\", max_iter=1000),\n",
    ")\n",
    "\n",
    "scores = cross_validate(\n",
    "    model_pipe,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    scoring=[\"f1\", \"accuracy\"],\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True\n",
    ")\n",
    "scores\n",
    "\n",
    "mean_scores_df = pd.concat(\n",
    "    [mean_scores_df, summarize_cv_scores(scores, \"Logistic Regression (feat eng.)\")]\n",
    ")\n",
    "mean_scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9 Interpretation\n",
    "rubric={accuracy:4,reasoning:2}\n",
    "\n",
    "1. Do you see any improvements with the new features compared to when you used only `CountVectorizer` features? Note that feature engineering is hard and requires domain expertise. If you do not see big improvements in scores with new features, that's OK. Do not get discouraged. The purpose of this exercise is to make you familiar to the process of extracting new features rather than getting the best scores. \n",
    "2. Show the first 20 coefficients with largest magnitudes and corresponding features. \n",
    "3. Examine the coefficients of the features we have extracted above. Do they make sense? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**solution_1_9_1**\n",
    "\n",
    "### YOUR ANSWER HERE\n",
    "\n",
    "No, we do not see any significant improvement using the new features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>coefficients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17206</th>\n",
       "      <td>thunderstorm</td>\n",
       "      <td>2.065141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16525</th>\n",
       "      <td>survived</td>\n",
       "      <td>2.057184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5892</th>\n",
       "      <td>died</td>\n",
       "      <td>2.012474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3731</th>\n",
       "      <td>carried</td>\n",
       "      <td>1.877849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>keyword_windstorm</td>\n",
       "      <td>1.847924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14735</th>\n",
       "      <td>scared</td>\n",
       "      <td>1.786221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16981</th>\n",
       "      <td>terrorists</td>\n",
       "      <td>1.732961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>keyword_collapse</td>\n",
       "      <td>-1.650739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4447</th>\n",
       "      <td>collision</td>\n",
       "      <td>1.631677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14260</th>\n",
       "      <td>road</td>\n",
       "      <td>1.625345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17939</th>\n",
       "      <td>ukrainian</td>\n",
       "      <td>1.612991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18845</th>\n",
       "      <td>warning</td>\n",
       "      <td>1.593274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15065</th>\n",
       "      <td>several</td>\n",
       "      <td>1.573139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13954</th>\n",
       "      <td>rescued</td>\n",
       "      <td>1.559296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19150</th>\n",
       "      <td>windstorm</td>\n",
       "      <td>1.534639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6558</th>\n",
       "      <td>ebola</td>\n",
       "      <td>1.530732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15066</th>\n",
       "      <td>severe</td>\n",
       "      <td>1.493350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5895</th>\n",
       "      <td>dies</td>\n",
       "      <td>1.482932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3691</th>\n",
       "      <td>car</td>\n",
       "      <td>1.482029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10466</th>\n",
       "      <td>love</td>\n",
       "      <td>-1.461361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                features  coefficients\n",
       "17206       thunderstorm      2.065141\n",
       "16525           survived      2.057184\n",
       "5892                died      2.012474\n",
       "3731             carried      1.877849\n",
       "217    keyword_windstorm      1.847924\n",
       "14735             scared      1.786221\n",
       "16981         terrorists      1.732961\n",
       "49      keyword_collapse     -1.650739\n",
       "4447           collision      1.631677\n",
       "14260               road      1.625345\n",
       "17939          ukrainian      1.612991\n",
       "18845            warning      1.593274\n",
       "15065            several      1.573139\n",
       "13954            rescued      1.559296\n",
       "19150          windstorm      1.534639\n",
       "6558               ebola      1.530732\n",
       "15066             severe      1.493350\n",
       "5895                dies      1.482932\n",
       "3691                 car      1.482029\n",
       "10466               love     -1.461361"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution_1_9_2\n",
    "\n",
    "### YOUR ANSWER HERE\n",
    "model_pipe.fit(X_train, y_train)\n",
    "\n",
    "cat_columns = (\n",
    "    pd.Series(\n",
    "        model_pipe.named_steps[\"columntransformer\"]\n",
    "        .named_transformers_[\"pipeline-2\"]\n",
    "        .named_steps[\"onehotencoder\"]\n",
    "        .get_feature_names()\n",
    "    )\n",
    "    .str.replace(\"x1_\", \"sentiment_\")\n",
    "    .str.replace(\"x0_\", \"keyword_\")\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "txt_columns = (\n",
    "    model_pipe.named_steps[\"columntransformer\"]\n",
    "    .named_transformers_[\"countvectorizer\"]\n",
    "    .get_feature_names()\n",
    ")\n",
    "\n",
    "features = numeric_features + binary_features + cat_columns + txt_columns\n",
    "coefficients = model_pipe.named_steps[\"logisticregression\"].coef_.flatten()\n",
    "\n",
    "model_interpret_df = pd.DataFrame(\n",
    "    data={\"features\": features, \"coefficients\": coefficients}\n",
    ")\n",
    "model_interpret_df = model_interpret_df.assign(\n",
    "    abs_coef=np.abs(model_interpret_df.coefficients)\n",
    ")\n",
    "model_interpret_df.sort_values(by=\"abs_coef\", ascending=False).head(20)[\n",
    "    [\"features\", \"coefficients\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**solution_1_9_3**\n",
    "\n",
    "### YOUR ANSWER HERE\n",
    "\n",
    "Some of the features and their corresponding coefficients make sense. For example, \n",
    "- we have a positive coefficient for negative sentiment and a negative coefficient for positive sentiment\n",
    "- `has_numbers` have a positive coefficient which confirms our intuition.\n",
    "\n",
    "However, the coefficients of engineered features are small indicating that their impact is not very significant in prediction. Hence, addition of these new features is not improving our validation score significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>coefficients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>n_words</td>\n",
       "      <td>-0.506661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rel_char_len</td>\n",
       "      <td>0.396628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>has_hyperlinks</td>\n",
       "      <td>-0.461376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>has_numbers</td>\n",
       "      <td>0.264000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>sentiment_compound</td>\n",
       "      <td>-0.553334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>sentiment_neg</td>\n",
       "      <td>0.290422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>sentiment_neu</td>\n",
       "      <td>0.345551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>sentiment_pos</td>\n",
       "      <td>-0.095927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               features  coefficients\n",
       "0               n_words     -0.506661\n",
       "1          rel_char_len      0.396628\n",
       "2        has_hyperlinks     -0.461376\n",
       "3           has_numbers      0.264000\n",
       "223  sentiment_compound     -0.553334\n",
       "224       sentiment_neg      0.290422\n",
       "225       sentiment_neu      0.345551\n",
       "226       sentiment_pos     -0.095927"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_interpret_df.loc[model_interpret_df.features.isin(\n",
    "    [\n",
    "        \"n_words\",\n",
    "        \"rel_char_len\",\n",
    "        \"has_hyperlinks\",\n",
    "        \"has_numbers\",\n",
    "        \"sentiment_compound\",\n",
    "        \"sentiment_neg\",\n",
    "        \"sentiment_neu\",\n",
    "        \"sentiment_pos\",\n",
    "    ]\n",
    "), [\"features\", \"coefficients\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.10 Test results\n",
    "rubric={accuracy:2, reasoning:2}\n",
    "\n",
    "**Yout tasks**\n",
    "\n",
    "1. Report f1 score on the test set with the model trained with all features. \n",
    "2. What additional time, other than prediction time, do we need if we are to use this model with our engineered features on the deployment data?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score on test set: 0.7339\n"
     ]
    }
   ],
   "source": [
    "# solution_1_10_1\n",
    "\n",
    "### YOUR ANSWER HERE\n",
    "\n",
    "y_pred = model_pipe.predict(X_test)\n",
    "print(f\"f1-score on test set: {f1_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**solution_1_10_2**\n",
    "\n",
    "### YOUR ANSWER HERE\n",
    "\n",
    "In addition to prediction time, we have to create the engineered feature on deployment dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:573]",
   "language": "python",
   "name": "conda-env-573-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
